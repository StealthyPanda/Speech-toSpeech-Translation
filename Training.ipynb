{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c66decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Hub models can be accessed @ https://pytorch.org/hub/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import talos\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Archs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68120cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128 + 3 # +3 cuz we use BOS, EOS and PAD tokens extra.\n",
    "d_model = 64\n",
    "dim_feedforward = 64\n",
    "num_encoder_layers=3\n",
    "num_decoder_layers=3\n",
    "max_seq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20cfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.573M params, 1.573M trainable params (100.0%), 6.291MB total in memory'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_model = create_cycle_transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    activation=F.relu,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_seq_length=max_seq_len\n",
    ")\n",
    "cycle_model.size_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8880b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class TokenBank(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            path : str, max_len : int = max_seq_len, n_samples : int = 50,\n",
    "            # start_index : int = 0, end_index : int = 10000,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        with open(path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.maxlen = max_len\n",
    "        self.data = data\n",
    "        self.ns = n_samples\n",
    "        # self.data = np.concatenate(list(map(lambda x: np.squeeze(x, 0), self.data_raw)))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.data), self.ns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = torch.tensor(self.data[idx], dtype=torch.int64)\n",
    "        sample = torch.concatenate([\n",
    "            torch.tensor([0 for _ in range(3)] + [1]),\n",
    "            sample[:max_seq_len - 8] + 3,\n",
    "            torch.tensor([2] + [0 for _ in range(3)]),\n",
    "        ])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "english_tb = TokenBank('eng_int_tokens.tokens')\n",
    "swahili_tb = TokenBank('swahili_int_tokens.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e236df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.573M params, 1.573M trainable params (100.0%), 6.291MB total in memory\n",
      "\u001b[32m\u001b[1mGPU(s) exist!\u001b[0m\n",
      "nbatches: 50\n",
      "Epoch 1/1 [=============================> ](98.00%) Took 04:1706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'closs': [5.283940315246582,\n",
       "  5.261687278747559,\n",
       "  5.1613850593566895,\n",
       "  5.096944808959961,\n",
       "  5.002572059631348,\n",
       "  5.121870517730713,\n",
       "  4.862157821655273,\n",
       "  5.0523810386657715,\n",
       "  4.975968837738037,\n",
       "  4.983453750610352,\n",
       "  4.8539862632751465,\n",
       "  5.231721878051758,\n",
       "  4.905221462249756,\n",
       "  4.855556964874268,\n",
       "  5.064143657684326,\n",
       "  4.819822788238525,\n",
       "  5.243703842163086,\n",
       "  4.935613632202148,\n",
       "  4.73769998550415,\n",
       "  4.868610382080078,\n",
       "  4.6972246170043945,\n",
       "  4.86647367477417,\n",
       "  5.003090858459473,\n",
       "  4.974726676940918,\n",
       "  4.984422206878662,\n",
       "  4.932287693023682,\n",
       "  4.790943622589111,\n",
       "  4.586535453796387,\n",
       "  4.599454879760742,\n",
       "  4.927395343780518,\n",
       "  4.8277130126953125,\n",
       "  5.060727596282959,\n",
       "  4.923603534698486,\n",
       "  4.884232521057129,\n",
       "  4.20780611038208,\n",
       "  4.966693878173828,\n",
       "  5.046350002288818,\n",
       "  4.6674370765686035,\n",
       "  4.622956275939941,\n",
       "  5.166489124298096,\n",
       "  5.343782901763916,\n",
       "  4.698246479034424,\n",
       "  5.09874153137207,\n",
       "  4.2636308670043945,\n",
       "  5.19062614440918,\n",
       "  4.298125743865967,\n",
       "  4.3332109451293945,\n",
       "  5.081976890563965,\n",
       "  5.062706470489502,\n",
       "  5.129599571228027],\n",
       " 'aloss': [0.5504394769668579,\n",
       "  0.5839742422103882,\n",
       "  0.6323899030685425,\n",
       "  0.6418004631996155,\n",
       "  0.6485599875450134,\n",
       "  0.6517544388771057,\n",
       "  0.641822099685669,\n",
       "  0.6476877331733704,\n",
       "  0.6316801905632019,\n",
       "  0.6671949028968811,\n",
       "  0.6582151651382446,\n",
       "  0.6733078360557556,\n",
       "  0.6753339171409607,\n",
       "  0.6773679852485657,\n",
       "  0.6798266768455505,\n",
       "  0.6823781132698059,\n",
       "  0.6749308109283447,\n",
       "  0.6747906804084778,\n",
       "  0.6864610910415649,\n",
       "  0.6848054528236389,\n",
       "  0.686905562877655,\n",
       "  0.6841838955879211,\n",
       "  0.6852740049362183,\n",
       "  0.6906582713127136,\n",
       "  0.6888288855552673,\n",
       "  0.6914682984352112,\n",
       "  0.6904323101043701,\n",
       "  0.6884139776229858,\n",
       "  0.6921541690826416,\n",
       "  0.690468966960907,\n",
       "  0.6924596428871155,\n",
       "  0.6920346021652222,\n",
       "  0.6924764513969421,\n",
       "  0.6927593946456909,\n",
       "  0.6928223967552185,\n",
       "  0.6928884983062744,\n",
       "  0.6927927732467651,\n",
       "  0.6929577589035034,\n",
       "  0.6930124759674072,\n",
       "  0.692796528339386,\n",
       "  0.6930354237556458,\n",
       "  0.6930813193321228,\n",
       "  0.692983865737915,\n",
       "  0.6930841207504272,\n",
       "  0.6929227709770203,\n",
       "  0.6930030584335327,\n",
       "  0.6931143999099731,\n",
       "  0.6931133270263672,\n",
       "  0.6931068897247314,\n",
       "  0.6931312680244446],\n",
       " 'dloss': [0.7265072464942932,\n",
       "  0.6363722085952759,\n",
       "  0.6303427219390869,\n",
       "  0.5607802271842957,\n",
       "  0.553457498550415,\n",
       "  0.5473737716674805,\n",
       "  0.5393801927566528,\n",
       "  0.5350461006164551,\n",
       "  0.5380582809448242,\n",
       "  0.5303066968917847,\n",
       "  0.5311235189437866,\n",
       "  0.538833498954773,\n",
       "  0.5225820541381836,\n",
       "  0.5345213413238525,\n",
       "  0.5180025100708008,\n",
       "  0.516005277633667,\n",
       "  0.5220350027084351,\n",
       "  0.5159030556678772,\n",
       "  0.5128960609436035,\n",
       "  0.5133864879608154,\n",
       "  0.5130075216293335,\n",
       "  0.5105709433555603,\n",
       "  0.5096030831336975,\n",
       "  0.5082373023033142,\n",
       "  0.5078374743461609,\n",
       "  0.5143383741378784,\n",
       "  0.5053582191467285,\n",
       "  0.5078179836273193,\n",
       "  0.5054007768630981,\n",
       "  0.5051395297050476,\n",
       "  0.5045718550682068,\n",
       "  0.5052886605262756,\n",
       "  0.5045261979103088,\n",
       "  0.5044165849685669,\n",
       "  0.5040087103843689,\n",
       "  0.5039865970611572,\n",
       "  0.5036712288856506,\n",
       "  0.5039252638816833,\n",
       "  0.5034806132316589,\n",
       "  0.5037893056869507,\n",
       "  0.503665566444397,\n",
       "  0.5035941004753113,\n",
       "  0.5035563707351685,\n",
       "  0.5034049153327942,\n",
       "  0.5035812854766846,\n",
       "  0.5034971833229065,\n",
       "  0.5032764673233032,\n",
       "  0.5033516883850098,\n",
       "  0.5033208131790161,\n",
       "  0.5032666921615601],\n",
       " 'lambda': [0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1,\n",
       "  0.1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            model : CycleTransformer,\n",
    "            eng_token_bank : torch.utils.data.Dataset,\n",
    "            swa_token_bank : torch.utils.data.Dataset,\n",
    "            pad_token : int = None,\n",
    "        ):\n",
    "        self.model = model\n",
    "        self.eng_bank = eng_token_bank\n",
    "        self.swa_bank = swa_token_bank\n",
    "        self.ptok = pad_token\n",
    "        \n",
    "        self.device = 'cuda' if talos.gpu_exists() else 'cpu'\n",
    "        \n",
    "        self.t1_t2opt = torch.optim.Adam(\n",
    "            list(self.model.ab_transformer.parameters()) +\n",
    "            list(self.model.ba_transformer.parameters())\n",
    "        )\n",
    "        self.t3opt = torch.optim.Adam(self.model.discriminator.parameters())\n",
    "        \n",
    "    \n",
    "    def move_to_device(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    \n",
    "    def train(self, epochs : int = 1, batch_size : int = 16, lam : float = 0.1):\n",
    "        eng_loader = torch.utils.data.DataLoader(self.eng_bank, batch_size, pin_memory=True)\n",
    "        swa_loader = torch.utils.data.DataLoader(self.swa_bank, batch_size, shuffle=True, pin_memory=True)\n",
    "        \n",
    "        self.move_to_device()\n",
    "        \n",
    "        nbatches = len(eng_loader)\n",
    "        print(f'nbatches: {nbatches}')\n",
    "        \n",
    "        losses = {\n",
    "            'closs' : [],\n",
    "            'aloss' : [],\n",
    "            'dloss' : [],\n",
    "            'lambda' : [],\n",
    "        }\n",
    "        \n",
    "        K = 30\n",
    "        \n",
    "        etime = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            prevtime = time.time()\n",
    "            for bi, o_eng_ids, o_swa_ids in zip(range(nbatches), eng_loader, swa_loader):\n",
    "                \n",
    "                o_eng_ids = o_eng_ids.to(self.device)\n",
    "                o_swa_ids = o_swa_ids.to(self.device)\n",
    "                \n",
    "                # outputs = self.model(o_eng_ids)\n",
    "                \n",
    "                p_swahili_ids, p_swahili_logits = self.model.forward_ab(o_eng_ids)\n",
    "                p_english_ids, p_english_logits = self.model.forward_ba(p_swahili_ids)\n",
    "                \n",
    "                d_fake = self.model.discriminator(torch.detach(p_swahili_ids))\n",
    "                d_real = self.model.discriminator(o_swa_ids)\n",
    "                \n",
    "                d_loss = discriminator_loss(d_real, d_fake)\n",
    "                \n",
    "                #update Discriminator\n",
    "                d_loss.backward()\n",
    "                self.t3opt.step()\n",
    "                \n",
    "                c_loss = cyclic_loss(\n",
    "                    p_english_logits,\n",
    "                    o_eng_ids,\n",
    "                    self.ptok\n",
    "                )\n",
    "                a_loss = adversarial_loss(self.model.discriminator(\n",
    "                    p_swahili_ids\n",
    "                ))\n",
    "                \n",
    "                #updating main pipeline\n",
    "                gen_loss = c_loss + (lam * a_loss)\n",
    "                gen_loss.backward()\n",
    "                self.t1_t2opt.step()\n",
    "                \n",
    "                losses['closs'].append(c_loss.detach().cpu().item())\n",
    "                losses['aloss'].append(a_loss.detach().cpu().item())\n",
    "                losses['dloss'].append(d_loss.detach().cpu().item())\n",
    "                losses['lambda'].append(lam)\n",
    "                \n",
    "                currtime = time.time()\n",
    "                eta = (currtime - prevtime) * (nbatches - bi)\n",
    "                eta = time.strftime('%M:%S', time.gmtime(eta))\n",
    "                f = bi/nbatches\n",
    "                lb = ('=' * int(f * K)) + '>' + (' ' * (K - int(f * K)))\n",
    "                print(\n",
    "                    f'\\rEpoch {epoch+1}/{epochs} [{lb}]({f*100:.2f}%) ETA <= {eta}', end=''\n",
    "                )\n",
    "                prevtime = time.time()\n",
    "            \n",
    "            cetime = time.time()\n",
    "            cetime = time.strftime(\"%M:%S\", time.gmtime(cetime - etime))\n",
    "            print(\n",
    "                f'\\rEpoch {epoch+1}/{epochs} [{lb}]({f*100:.2f}%) Took {cetime}'\n",
    "            )\n",
    "            etime = time.time()\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "jarvis = create_cycle_transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    activation=F.relu,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_seq_length=max_seq_len\n",
    ")\n",
    "print(jarvis.size_info)\n",
    "friday = Trainer(jarvis, english_tb, swahili_tb)\n",
    "hist = friday.train(epochs=10, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
